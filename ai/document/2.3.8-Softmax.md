### 2.3.8 softmax
到目前为止，我们的神经网络都是使用着二元分类。所以神经网络只能判断是或否，例如只能识别图片里是有猫还是没有猫.本篇文章中我们将要学习到的softmax可以让神经网络变得更加强大，它可以让神经网络判断多种分类。

例如，我们可以利用softmax构建出能够同时识别猫和狗以及小鸡的神经网络。之前我们是用 1 代表有猫，用 0 代表没有猫，现在我们可以用 3 代表有小鸡，用 2 代表狗，用 1 代表猫，用 0 代表其它。如下图所示。

<table>
    <tr>
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/bfa54638-a309-444a-ac0b-6f3614678f76.png"></td>
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/a2c66520-462e-462d-b7f4-5d5cbeb91e33.png"></td>
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/8460482a-1738-446a-b84f-d280d22debd5.png"></td>
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/e4d3b032-0c46-4c02-8d70-adc46e30f0b8.png"></td>
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/221e46d2-1e8f-49ed-b19b-1b42960ceb4d.png"></td> 
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/0e6a5932-66eb-4a92-9346-b3730006430b.png"></td> 
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/0a84da91-31ec-4f4f-bbf5-6185cea37578.png"></td> 
        <td><img style='width:100px;height:100px' src="https://imgkr.cn-bj.ufileos.com/0c981649-1599-4f39-9c4a-6cd557c531f8.png"></td> 
    </tr>
    <tr align='center'><td>3</td><td>1</td><td>2</td><td>0</td><td>3</td><td>2</td><td>0</td><td>1</td></tr>
</table>

为了实现上面的功能，我们将输出层的神经元数量设置为 4，因为我们要判断 4 个类别。如果类别数量用大写的 C 来表示的话，那么本例中 C 就等于 4。下面的神经网络中，输出层就有四个神经元。

<img src="svg/2.3.8-01.svg"/>

我们希望这 4 个神经元可以给出这4个分类的概率。例如我们希望第一个神经元会给出其它分类的概率，第二个神经元会给出输入图像中有猫的概率，第三个会给出狗的概率，第四个是小鸡的概率。这四个概率加起来等于 1。

在这个输出层上面我们要应用一种新的激活函数--softmax 激活函数。与之前我们学过的激活函数一样，函数的输入是 z。softmax 激活函数的实现主要分为两步。

第一步，我们先用 z 算出一个临时变量 t，$t=e^z$。这里的 z 表示的是输出层四个神经元相关的 z，所以这里的 z 是一个(4，1)维的向量，所以计算结果 t 也是(4，1)维的向量。

第二步，用 z 算出 a。a=$\frac{t}{np.sum(t)}$。这里稍微有一点点巧妙，难于理解。t 是一个向量，np.sum(t)的结果是一个数值，这个数值是 t 向量中所有元素的值的总和，所以 $\frac{t}{np.sum(t)}$会自动发生 python 广播化操作，所以 a 也将是一个向量，并且 a 向量里每一个元素就是 t 向量中每一个元素在整个 t 向量中的占比，也就是说 a 向量里所有元素加起来的总和将会是 1。如果你不理解这段话，那么建议你再复习一下 python 广播化(broadcast)。

上面的解释还是比较难理解的，我给大家举一个实例吧。假设 z 为如下的向量。

$$z = \begin{bmatrix}5\\2\\-1\\3\end{bmatrix}$$

t 的结果将如下所示。

$$t = \begin{bmatrix}e^5\\e^2\\e^-1\\e^3\end{bmatrix} = \begin{bmatrix}148.4\\3.4\\0.4\\20.5\end{bmatrix}$$

t 的所有元素相加起来等于176.3。所以 a 的结果将如下所示。

$$a = \begin{bmatrix}148.4/176.3\\3.4/176.3\\0.4/176.3\\20.5/176.3\end{bmatrix}$$

前面我们学过的激活函数都是一次只激活一个神经元，而 softmax 在本例中一次性激活了四个神经元。

下面我给出一些图来加深大家对 softmax 更加直观的认识。为了便于理解，我们假设只有两个特征 $x_1$和 $x_2$，而且没有隐藏层，即只有输入层和输出层。因为没有隐藏层，所以下面的分类线条都是线性的。当然，如果有了复杂的隐藏层，那么分类的线条会更加复杂。下面分别给出了 C 等于 4，5，6 的情况。

<img align='center' style='width:80%' src='https://imgkr.cn-bj.ufileos.com/f80d1332-bf8c-4694-af82-ee1fa68c7e24.png'/>
### 2.3.9 深入理解Softmax
通过上一篇文章的学习，我们知道 $softmax$ 会先利用输出层的z向量生成一个临时向量 $t$，然后利用 $t$ 生成 $a$。

$$
z = \begin{bmatrix}\color{red}5\\2\\-1\\3\end{bmatrix}\quad
t = \begin{bmatrix}148.4\\3.4\\0.4\\20.5\end{bmatrix}\quad
a = \begin{bmatrix}\color{red}{0.842}\\0.042\\0.002\\0.114\end{bmatrix}\quad
$$

因为 $z$ 的第一个元素最大，是 5，所以相应的 $a$ 的第一个元素也是最大的，是 0.842，表示输入图片是第一个分类的概率最大。在上一篇文章的举例中，第一个元素我们将其假设为代表其它分类的概率，所以 0.842 表示输入图片是其它分类的概率高达 84.2%。

为什么将其命名为 $softmax$ 呢? 因为以前有一个技术叫做 $hardmax$。$softmax$ 会将 $a$ 变成概率，而 $hardmax$ 会将 $a$ 直接变成只有一个元素是 1 ，其余元素都是 0 的向量。在我们的例子中，第一个分类的概率最大，所以第一个元素是 1，其余的都是 0。比起 $hardmax$ 来说，$softmax$ 更加“柔和”一些，没有那么“强硬”，因此得名 $softmax$。

在实际应用中，我们会将标签 $y$ 定义成只有一个元素是 1 的向量。下面的向量中第二个元素是 1，表示该标签对应的输入图像属于第二个分类。

$y = \begin{bmatrix}0\\\color{blue}1\\0\\0\end{bmatrix}$

我们的 $y'$ (输出层的 $a$)是一个概率的向量。下面的向量中第二个元素才 0.2，而且并不是最大的，这种情况就说明预测得不准确。第一个元素反而最大，说明神经网络错误地把输入图像预测成了第一个分类。

$y' = a = \begin{bmatrix}0.4\\\color{blue}{0.2}\\0.1\\0.3\end{bmatrix}$

我们的损失函数也发生了变化。变成了如下形式。下式中的 $c$ 是类别的数量，在我们的例子中类别数量是 4。

$L(y',y) = -\sum_{j = 1}^{c}y_jlog(y'_j)$

下面我将前面的实际数字带入到上式中，来帮助大家理解上面的损失函数公式。因为上面的 $y$ 向量中只有一个元素是 1 (第二个元素)，其他元素是 0，所以将它带入到上面的公式中，损失函数 $-\sum_{j = 1}^{c}y_jlog(y'_j)$ 就变成了 $-log(y'_2)$ 。梯度下降的目的就是要调整参数 $w$ 和 $b$ 让损失函数最小，在本例中也就是要让 $-log(y'_2)$ 最小。而想要让 $-log(y'_2)$ 最小，就是要让 $y'_2$ 最大。$y'_2$ 是与真实标签的第二个元素相对应的，$y'_2$ 越大就说明预测越精准。所以通过这个损失函数，梯度下降就会-步步地让损失函数越来越小，也就是一步步地调整参数使 $y'_2$ 越来越大，使预测值越来越接近真实标签值。

上面我们说的是损失函数。那么成本函数呢? 成本函数只需要将每一个样本的损失函数累加起来，然后求得一个平均值。公式如下。
$J = \frac{1}{m}\sum_{i = 1}^{m}L(y'^{(i)},y^{(i)})$
### 归一化隐藏层
还记得在2.1.7的时候，我们学习了将输入特征进行归一化处理。处理后可以大大提升神经网络的学习效率。同理，我们也可以对隐藏层中的 $z$ 进行归一化处理。这个还是比较好理解的，因为第一层的 $z$ 就可以看作是第二层的输入特征。

根据 2.1.7 里学到的知识，我们可以对 $z$ 进行如下的归一化处理。

#### <center> 第一步就是使 z 的平均值成为 0 </center>
### $$u = \frac{1}{m} \sum_{i = 1}^{m} z^{(i)}$$
### $$z = z - u$$
#### <center> 第二步是使z的方差变成 1</center>
### $$\sigma = \frac{1}{m} \sum_{i = 1}^{m} (z^{(i)})^2$$
### $$z = \frac{z - u}{\sqrt{\sigma + \varepsilon}}$$
你可能发现 2.1.7 里是 $z = z - σ$ 。是的，对隐藏层中 $z$ 的归一化与对数据集输入特征的归一化流程是不同的。在 2.1.7 中，归一化只有上面两个步骤。而对隐藏层的归一化还有额外的步骤。

经过上面两个步骤后， $z$ 数据的分布状况发生了变化，这是我们希望的;但是 $z$ 的平均值变成了 0，方差变成了 1，这是我们并不希望。我们希望 $z$ 可以任意分布在坐标系的任意位置，而不是总在坐标中间。

所以我们需要第三步，使 $z$ 的分布区域可以被任意挪动。
### $$z = \gamma{z} + \beta$$
上式中的 $𝛾$ 和 $β$ 是像 $w$ 和 $b$ 一样的参数，是神经网络可以根据学习而不断改变优化的参数，即神经网络会不断地优化 $z$ 的分布位置。 $𝛾$ 可以控制方差， $β$ 可以控制平均值。举个极端的例子你们就明白了，假设将 $𝛾$ 设置为$\sqrt{σ+ε}$，并且将 $β$ 设置为 $u$ ，那么大家结合第二步和第二步的公式就会发现它们对 $z$ 的处理都被抵消了，也就是说，通过将 $𝛾$ 和 $β$ 设置为上面的极端值后就撤销了对 $z$ 的平均值和方差的修改。同理，通过设置 $𝛾$ 和 $β$ 的值就可以为 $z$ 设置任意的平均值和方差。

因为 $z = wx + b$ ，而在第三步中又有了 $+β$ 的操作，所以我们就可以去掉 $b$ 参数了。也就是说只有 $w$ 和 $𝛾$ 以及 $β$ 三个参数了。

在神经网络中，我们会为数据集进行归一化处理，然后又会为第一层进行 $z$ 的归一化处理，然后又会为第二层进行 $z$ 的归一化处理....
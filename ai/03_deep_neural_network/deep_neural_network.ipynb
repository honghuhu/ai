{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"deep_neural_network.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"rPi3n4mEf9Sf"},"source":["本次实战我们构建一个深度神经网络，并用它来完成和《第一个人工智能程序》时同样的任务——识别图片中是否有猫。在《第一个人工智能程序》时，我们使用的是单神经元神经网络，本次我们使用深度神经网络来提升预测精准度。"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Ho_9jbBof9Sh","executionInfo":{"status":"error","timestamp":1625301193789,"user_tz":-480,"elapsed":11,"user":{"displayName":"yuxiang wang","photoUrl":"","userId":"09333219890846106757"}},"outputId":"c34e7705-8b2e-4d40-9d57-9e4afee405d6","colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","# 加载我们自定义的工具函数\n","from testCases import *\n","from dnn_utils import *\n","\n","# 设置一些画图相关的参数\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) \n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","np.random.seed(1)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3886da55b0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 加载我们自定义的工具函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtestCases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'testCases'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"XtYxUemPf9Sj"},"source":["首先，我们将构建深度神经网络所需的工具函数一个个给编写好。"]},{"cell_type":"code","metadata":{"id":"vSYpFMXgf9Sj"},"source":["# 该函数用于初始化所有层的参数w和b\n","def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    参数:\n","    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n","    例如，layer_dims=[5,4,3]，表示第一层有5个神经元，第二层有4个，最后一层有3个神经元\n","    \n","    返回值:\n","    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n","    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n","    \"\"\"\n","#     np.random.seed(1)\n","    parameters = {}\n","    L = len(layer_dims) # 获取神经网络总共有几层\n","\n","    # 遍历每一层，为每一层的W和b进行初始化\n","    for l in range(1, L):\n","        # 构建并随机初始化该层的W。由我前面的文章《1.4.3 核对矩阵的维度》可知，Wl的维度是(n[l] , n[l-1]) \n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n","        # 构建并初始化b\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        # 核对一下W和b的维度是我们预期的维度\n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","    #就是利用上面的循环，我们就可以为任意层数的神经网络进行参数初始化，只要我们提供每一层的神经元个数就可以了。       \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8uANwqef9Sj","outputId":"e01e8302-8a55-43fb-ba4b-ed148b393bef"},"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n"," [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n"," [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n"," [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n"," [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n"," [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mlowbiVhf9Sl"},"source":["下面开始构建前向传播所需的工具函数"]},{"cell_type":"markdown","metadata":{"id":"xC-KZDJ3f9Sl"},"source":["下面的linear_forward用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，这个称之为线性前向传播"]},{"cell_type":"code","metadata":{"id":"XiSR0Gfof9Sl"},"source":["def linear_forward(A, W, b):   \n","    Z = np.dot(W, A) + b\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnbNg3H0f9Sm","outputId":"05d99f98-54a5-425c-ab41-76f2af6a4d0a"},"source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Z = [[ 3.26295337 -1.23429987]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LwZNQXT0f9Sn"},"source":["下面的linear_activation_forward用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数，使用了激活函数之后上面的线性前向传播就变成了非线性前向传播了。在dnn_utils.py中我们自定义了两个激活函数，sigmoid和relu。它们都会根据传入的Z计算出A。"]},{"cell_type":"code","metadata":{"id":"W6RQqm01f9Sn"},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Arguments:\n","    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n","    W -- 本层相关的W\n","    b -- 本层相关的b\n","    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n","    \"\"\"\n","    \n","    Z, linear_cache = linear_forward(A_prev, W, b)\n","    \n","    if activation == \"sigmoid\": # 如果该层使用sigmoid        \n","        A = sigmoid(Z) \n","    elif activation == \"relu\":\n","        A = relu(Z)\n","        \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcX0r6Sef9Sn","outputId":"fe38925f-8fda-478f-d3ce-6475c1b1b6dc"},"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a8UPM0T5f9So"},"source":["# 这个函数构建了一个完整的前向传播过程。这个前向传播一共有L层，前面的L-1层用的激活函数是relu，最后一层使用sigmoid。\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","    参数:\n","    X -- 输入的特征数据\n","    parameters -- 这个list列表里面包含了每一层的参数w和b\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    \n","    # 获取参数列表的长度，这个长度的一半就是神经网络的层数。\n","    # 为什么是一半呢？因为列表是这样的[w1,b1,w2,b2...wl,bl],里面的w1和b1代表了一层\n","    L = len(parameters) // 2  \n","    \n","    # 循环L-1次，即进行L-1步前向传播，每一步使用的激活函数都是relu\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev,\n","                                             parameters['W' + str(l)], \n","                                             parameters['b' + str(l)],\n","                                             activation='relu')\n","        caches.append(cache)# 把一些变量数据保存起来，以便后面的反向传播使用\n","        \n","    \n","    # 进行最后一层的前向传播，这一层的激活函数是sigmoid。得出的AL就是y'预测值\n","    AL, cache = linear_activation_forward(A, \n","                                          parameters['W' + str(L)], \n","                                          parameters['b' + str(L)], \n","                                          activation='sigmoid')\n","    caches.append(cache)\n","   \n","    assert(AL.shape == (1, X.shape[1]))\n","            \n","    return AL, caches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bTQyTcTf9So","outputId":"dae1f4a9-ad76-480c-f0e4-166edd347730"},"source":["X, parameters = L_model_forward_test_case()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AL = [[0.17007265 0.2524272 ]]\n","Length of caches list = 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wxiTtUoyf9Sp"},"source":["# 上面已经完成了前向传播了。下面这个函数用于计算成本（单个样本时是损失，多个样本时是成本）。\n","# 通过每次训练的成本我们就可以知道当前神经网络学习的程度好坏。\n","def compute_cost(AL, Y):\n","       \n","    m = Y.shape[1]\n","    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n","    \n","    cost = np.squeeze(cost)# 确保cost是一个数值而不是一个数组的形式\n","    assert(cost.shape == ())\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKM4QNlkf9Sp","outputId":"c13b6b02-f24e-42f0-836a-e2f73dce4618"},"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cost = 0.41493159961539694\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IDKi5Zkjf9Sq"},"source":["上面已经实现了前向传播和成本函数，下面开始实现反向传播。通过反向传播来计算梯度——计算每层的w和b相当于成本函数的偏导数。"]},{"cell_type":"markdown","metadata":{"id":"emkZTYfwf9Sq"},"source":["下面的linear_backward函数用于根据后一层的dZ来计算前面一层的dW，db和dA。也就是实现了下面3个公式\n","$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n","$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n","$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"]},{"cell_type":"code","metadata":{"id":"tNOSR0Cif9Sq"},"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    参数:\n","    dZ -- 后面一层的dZ\n","    cache -- 前向传播时我们保存下来的关于本层的一些变量\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = np.dot(dZ, cache[0].T) / m\n","    db = np.sum(dZ, axis=1, keepdims=True) / m\n","    dA_prev = np.dot(cache[1].T, dZ)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljWZEcpnf9Sq","outputId":"5a9ba7e5-aad2-46aa-bd35-7ca441f3a570"},"source":["dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dA_prev = [[ 0.51822968 -0.19517421]\n"," [-0.40506361  0.15255393]\n"," [ 2.37496825 -0.89445391]]\n","dW = [[-0.10076895  1.40685096  1.64992505]]\n","db = [[0.50629448]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MNA2pbYVf9Sr"},"source":["下面的linear_activation_backward用于根据本层的dA计算出本层的dZ。就是实现了下面的公式\n","$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n","上式的g'()表示求Z相当于本层的激活函数的偏导数。所以不同的激活函数也有不同的求导公式。\n","我们为大家编写了两个求导函数sigmoid_backward和relu_backward。大家当前不需要关心这两个函数的内部实现，当然，如果你感兴趣可以到dnn_utils.py里面去看它们的实现。"]},{"cell_type":"code","metadata":{"id":"po87z9s9f9Sr"},"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    参数:\n","    dA -- 本层的dA \n","    cache -- 前向传播时保存的本层的相关变量\n","    activation -- 指示该层使用的是什么激活函数: \"sigmoid\" 或 \"relu\"\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)        \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","    \n","    # 这里我们又顺带根据本层的dZ算出本层的dW和db以及前一层的dA\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGn7_muRf9Sr","outputId":"7f4b7c61-6107-4698-cd4f-9706ebe3fd57"},"source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9zGJmDkXf9Sr"},"source":["# 下面这个函数构建出整个反向传播。\n","def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    参数:\n","    AL -- 最后一层的A，也就是y'，预测出的标签\n","    Y -- 真实标签\n","    caches -- 前向传播时保存的每一层的相关变量，用于辅助计算反向传播\n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # 获取神经网络层数。caches列表的长度就等于神经网络的层数\n","    Y = Y.reshape(AL.shape) # 让真实标签的维度和预测标签的维度一致\n","    \n","    # 计算出最后一层的dA，前面文章我们以及解释过，最后一层的dA与前面各层的dA的计算公式不同，\n","    # 因为最后一个A是直接作为参数传递到成本函数的，所以不需要链式法则而直接就可以求dA（A相当于成本函数的偏导数）\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # 计算最后一层的dW和db，因为最后一层使用的激活函数是sigmoid\n","    current_cache = caches[-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n","                                                                                            dAL, \n","                                                                                            current_cache,\n","                                                                                            activation = \"sigmoid\")\n","\n","    # 计算前面L-1层到第一层的每层的梯度，这些层都使用relu激活函数\n","    for c in reversed(range(1,L)): # reversed(range(1,L))的结果是L-1,L-2...1。是不包括L的。第0层是输入层，不必计算。\n","        # 这里的c表示当前层\n","        grads[\"dA\" + str(c-1)], grads[\"dW\" + str(c)], grads[\"db\" + str(c)] = linear_activation_backward(\n","            grads[\"dA\" + str(c)], \n","            caches[c-1],\n","            # 这里我们也是需要当前层的caches，但是为什么是c-1呢？因为grads是字典，我们从1开始计数，而caches是列表，\n","            # 是从0开始计数。所以c-1就代表了c层的caches。数组的索引很容易引起莫名其妙的问题，大家编程时一定要留意。\n","            activation = \"relu\")\n","\n","    return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8vwE9c2f9Ss","outputId":"468610b1-7fb1-4497-b0c2-0fcc521cd580"},"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print (\"dW1 = \"+ str(grads[\"dW1\"]))\n","print (\"db1 = \"+ str(grads[\"db1\"]))\n","print (\"dA1 = \"+ str(grads[\"dA1\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n"," [0.         0.         0.         0.        ]\n"," [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n","db1 = [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]]\n","dA1 = [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SifNVfBzf9Ss"},"source":["通过上面的反向传播，我们得到了每一层的梯度（每一层w和b相当于成本函数的偏导数）。下面的update_parameters函数将利用这些梯度来更新/优化每一层的w和b，也就是进行梯度下降。\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"]},{"cell_type":"code","metadata":{"id":"yCV83rgKf9Ss"},"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Arguments:\n","    parameters -- 每一层的参数w和b \n","    grads -- 每一层的梯度\n","    learning_rate -- 是学习率，学习步进\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # 获取层数。//除法可以得到整数\n","\n","    for l in range(1,L+1):\n","        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n","        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CiyqJJ4f9St","outputId":"116a1e57-8689-48c4-f8c2-f60c96c9a6f4"},"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \" + str(parameters[\"W1\"]))\n","print (\"b1 = \" + str(parameters[\"b1\"]))\n","print (\"W2 = \" + str(parameters[\"W2\"]))\n","print (\"b2 = \" + str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GoPwYL7Hf9St"},"source":["至此，工具函数都编写好了，下面我们开始使用它们来构建一个深度神经网络以识别图片中是否有猫。\n","首先，我们还是老规矩——第一步加载数据集。本次的数据集和《第一个人工智能程序》时用的是一样一样的。\n","关于数据集的信息和加载方式可以看我的《第一个人工智能程序》"]},{"cell_type":"code","metadata":{"id":"qqZ8yZkQf9St"},"source":["train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n","\n","m_train = train_x_orig.shape[0] # 训练样本的数量\n","m_test = test_x_orig.shape[0] # 测试样本的数量\n","num_px = test_x_orig.shape[1] # 每张图片的宽/高\n","\n","# 为了方便后面进行矩阵运算，我们需要将样本数据进行扁平化和转置\n","# 处理后的数组各维度的含义是（图片数据，样本数）\n","train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n","test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n","\n","# 下面我们对特征数据进行了简单的标准化处理（除以255，使所有值都在[0，1]范围内）\n","train_x = train_x_flatten/255.\n","test_x = test_x_flatten/255."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fQpRMZ9f9Su"},"source":["# 利用上面的工具函数构建一个深度神经网络训练模型\n","def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n","    \"\"\"    \n","    参数:\n","    X -- 数据集\n","    Y -- 数据集标签\n","    layers_dims -- 指示该深度神经网络用多少层，每层有多少个神经元\n","    learning_rate -- 学习率\n","    num_iterations -- 指示需要训练多少次\n","    print_cost -- 指示是否需要在将训练过程中的成本信息打印出来，好知道训练的进度好坏。\n","    \n","    返回值:\n","    parameters -- 返回训练好的参数。以后就可以用这些参数来识别新的陌生的图片\n","    \"\"\"\n","\n","    np.random.seed(1)\n","    costs = []                  \n","\n","    # 初始化每层的参数w和b\n","    parameters = initialize_parameters_deep(layers_dims)\n","    \n","    # 按照指示的次数来训练深度神经网络\n","    for i in range(0, num_iterations):\n","        # 进行前向传播\n","        AL, caches = L_model_forward(X, parameters)\n","        # 计算成本\n","        cost = compute_cost(AL, Y)\n","        # 进行反向传播\n","        grads = L_model_backward(AL, Y, caches)\n","        # 更新参数，好用这些参数进行下一轮的前向传播\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        # 打印出成本\n","        if i % 100 == 0:\n","            if print_cost and i > 0:\n","                print (\"训练%i次后成本是: %f\" % (i, cost))\n","            costs.append(cost)\n","            \n","    # 画出成本曲线图\n","    plt.plot(np.squeeze(costs))\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (per tens)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv04pDEKf9Su","outputId":"e08e25f5-fa75-4fec-bb49-9d4bcb753e31"},"source":["# 设置好深度神经网络的层次信息——下面代表了一个4层的神经网络（12288是输入层），\n","# 第一层有20个神经元，第二层有7个神经元。。。\n","# 你也可以构建任意层任意神经元数量的神经网络，只需要更改下面这个数组就可以了\n","layers_dims = [12288, 20, 7, 5, 1]\n","\n","# 根据上面的层次信息来构建一个深度神经网络，并且用之前加载的数据集来训练这个神经网络，得出训练后的参数\n","parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["训练100次后成本是: 0.672053\n","训练200次后成本是: 0.648263\n","训练300次后成本是: 0.611507\n","训练400次后成本是: 0.567047\n","训练500次后成本是: 0.540138\n","训练600次后成本是: 0.527930\n","训练700次后成本是: 0.465477\n","训练800次后成本是: 0.369126\n","训练900次后成本是: 0.391747\n","训练1000次后成本是: 0.315187\n","训练1100次后成本是: 0.272700\n","训练1200次后成本是: 0.237419\n","训练1300次后成本是: 0.199601\n","训练1400次后成本是: 0.189263\n","训练1500次后成本是: 0.161189\n","训练1600次后成本是: 0.148214\n","训练1700次后成本是: 0.137775\n","训练1800次后成本是: 0.129740\n","训练1900次后成本是: 0.121225\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"vUEPlpvBf9Su"},"source":["下面我们再编写一个预测函数，来用上面训练得到的参数来进行预测"]},{"cell_type":"code","metadata":{"id":"MHFKOPvwf9Su"},"source":["def predict(X,parameters):   \n","    m = X.shape[1]\n","    n = len(parameters) // 2 # number of layers in the neural network\n","    p = np.zeros((1,m))\n","    \n","    # 进行一次前向传播，得到预测结果\n","    probas, caches = L_model_forward(X, parameters)\n","    print(probas)\n","    # 将预测结果转化成0和1的形式，即大于0.5的就是1，否则就是0\n","    for i in range(0, probas.shape[1]):\n","        if probas[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","        \n","    return p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lm97YIOvf9Su","outputId":"8ee8fc39-35bd-49a1-d363-d80c681255e6"},"source":["# 对训练数据集进行预测\n","pred_train = predict(train_x,parameters)\n","print(\"预测准确率是: \" + str(np.sum((pred_train == train_y) / train_x.shape[1])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.11355187 0.11321288 0.99421715 0.11320249 0.11151952 0.11320249\n","  0.11320249 0.99868565 0.11297217 0.11320249 0.11320249 0.99212492\n","  0.11361743 0.99978505 0.99238936 0.11223783 0.11320249 0.11320249\n","  0.11273694 0.99979337 0.11320249 0.11113007 0.11321129 0.11336202\n","  0.99805632 0.99272884 0.11158027 0.99875585 0.11320249 0.98798578\n","  0.11340662 0.11231844 0.11339752 0.11237601 0.11320249 0.11320488\n","  0.11320249 0.11305355 0.95459455 0.11634576 0.11320249 0.97668361\n","  0.99827849 0.11320249 0.11320249 0.11209139 0.11292575 0.98122606\n","  0.11320249 0.11309981 0.96476361 0.11320249 0.11320249 0.11328443\n","  0.99710096 0.11320249 0.99809563 0.98289964 0.11320249 0.99937053\n","  0.99667978 0.98935349 0.11320249 0.11320249 0.11320249 0.11378944\n","  0.11269344 0.11198864 0.11320249 0.11320249 0.11320249 0.99997463\n","  0.11320249 0.11229775 0.11320249 0.15924221 0.11320249 0.11320249\n","  0.11320249 0.11320249 0.11320249 0.11394959 0.11320249 0.99569335\n","  0.98801701 0.11320249 0.11374373 0.11320249 0.97048389 0.11320249\n","  0.11091985 0.1133149  0.11320249 0.99158351 0.99954149 0.11320249\n","  0.11320249 0.98219517 0.11320249 0.11273604 0.11384284 0.11320249\n","  0.99523984 0.11621143 0.99976858 0.11320249 0.99926581 0.95314957\n","  0.96919655 0.99469097 0.99582645 0.99940756 0.11320249 0.11320249\n","  0.11320249 0.11320249 0.11320249 0.99243892 0.11320249 0.11320249\n","  0.11320249 0.95951483 0.11320249 0.11320249 0.27699502 0.11320249\n","  0.99196707 0.11341575 0.99018699 0.99896568 0.12543501 0.11170539\n","  0.11320249 0.99984857 0.98101634 0.9856538  0.99569001 0.97622991\n","  0.11320249 0.11320249 0.11320249 0.11320249 0.99837671 0.11320249\n","  0.98912318 0.99179754 0.99079338 0.11348789 0.98268212 0.99753093\n","  0.11834557 0.11261488 0.11320249 0.99815318 0.11366585 0.11320249\n","  0.99824494 0.11320249 0.11320249 0.11275918 0.11320249 0.11320249\n","  0.99449718 0.11320249 0.99780506 0.11320249 0.99972335 0.11320249\n","  0.11287302 0.99061965 0.98985401 0.99884966 0.11607315 0.11320249\n","  0.99932897 0.99553378 0.11212132 0.99040699 0.11119629 0.9851028\n","  0.11229247 0.11311966 0.11320249 0.11320249 0.11320249 0.98457229\n","  0.11320249 0.11092261 0.9999191  0.11320249 0.11320249 0.11345908\n","  0.99178288 0.11320249 0.11320249 0.11300832 0.12323144 0.99378729\n","  0.11205717 0.11320249 0.37845867 0.11333768 0.11320249 0.11320249\n","  0.11316266 0.11098908 0.11320249 0.11320249 0.11218756]]\n","预测准确率是: 0.9808612440191385\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nU2AsvAtf9Sv","outputId":"8445b733-64ea-427b-f070-b5fb5957e833"},"source":["# 对测试数据集进行预测\n","pred_test = predict(test_x,parameters)\n","print(\"预测准确率是: \"  + str(np.sum((pred_test == test_y) / test_x.shape[1])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.99857185 0.9958758  0.97549937 0.81043887 0.97941449 0.54017501\n","  0.78451331 0.99878998 0.98025811 0.98951161 0.9952642  0.4091547\n","  0.98829682 0.99910113 0.11320249 0.99766381 0.26173268 0.99079338\n","  0.80657707 0.13828897 0.9813595  0.12434335 0.11320249 0.74828188\n","  0.95205147 0.98900063 0.94263045 0.11320249 0.21053047 0.99547312\n","  0.95271361 0.99978218 0.99466482 0.97009512 0.83908259 0.11320249\n","  0.12638741 0.6924672  0.91541111 0.11320249 0.93824396 0.96909619\n","  0.82745341 0.11345511 0.95550664 0.99630299 0.55384169 0.99997405\n","  0.60810654 0.19278617]]\n","预测准确率是: 0.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"REEXlcAuf9Sv"},"source":["可以看到，比我们之前的的单神经元网络的0.7提升了0.1。不要小看这0.1，这是很难提升的。别说是0.1，如果全世界都只能做到0.9，如果你能提升0.05，那么你就享誉全球啦！\n","\n","当然，0.8还不是我们的极限，后面的文章我会带领大家继续提升它。"]}]}
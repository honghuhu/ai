{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实战我们构建一个深度神经网络，并用它来完成和《第一个人工智能程序》时同样的任务——识别图片中是否有猫。在《第一个人工智能程序》时，我们使用的是单神经元神经网络，本次我们使用深度神经网络来提升预测精准度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载我们自定义的工具函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些画图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将构建深度神经网络所需的工具函数一个个给编写好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数用于初始化所有层的参数w和b\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n",
    "    例如，layer_dims=[5,4,3]，表示第一层有5个神经元，第二层有4个，最后一层有3个神经元\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n",
    "    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n",
    "    \"\"\"\n",
    "#     np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # 获取神经网络总共有几层\n",
    "\n",
    "    # 遍历每一层，为每一层的W和b进行初始化\n",
    "    for l in range(1, L):\n",
    "        # 构建并随机初始化该层的W。由我前面的文章《1.4.3 核对矩阵的维度》可知，Wl的维度是(n[l] , n[l-1]) \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n",
    "        # 构建并初始化b\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # 核对一下W和b的维度是我们预期的维度\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    #就是利用上面的循环，我们就可以为任意层数的神经网络进行参数初始化，只要我们提供每一层的神经元个数就可以了。       \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始构建前向传播所需的工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_forward用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，这个称之为线性前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):   \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_forward用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数，使用了激活函数之后上面的线性前向传播就变成了非线性前向传播了。在dnn_utils.py中我们自定义了两个激活函数，sigmoid和relu。它们都会根据传入的Z计算出A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\": # 如果该层使用sigmoid        \n",
    "        A = sigmoid(Z) \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数构建了一个完整的前向传播过程。这个前向传播一共有L层，前面的L-1层用的激活函数是relu，最后一层使用sigmoid。\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 这个list列表里面包含了每一层的参数w和b\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    # 获取参数列表的长度，这个长度的一半就是神经网络的层数。\n",
    "    # 为什么是一半呢？因为列表是这样的[w1,b1,w2,b2...wl,bl],里面的w1和b1代表了一层\n",
    "    L = len(parameters) // 2  \n",
    "    \n",
    "    # 循环L-1次，即进行L-1步前向传播，每一步使用的激活函数都是relu\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)# 把一些变量数据保存起来，以便后面的反向传播使用\n",
    "        \n",
    "    \n",
    "    # 进行最后一层的前向传播，这一层的激活函数是sigmoid。得出的AL就是y'预测值\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面已经完成了前向传播了。下面这个函数用于计算成本（单个样本时是损失，多个样本时是成本）。\n",
    "# 通过每次训练的成本我们就可以知道当前神经网络学习的程度好坏。\n",
    "def compute_cost(AL, Y):\n",
    "       \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)# 确保cost是一个数值而不是一个数组的形式\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面已经实现了前向传播和成本函数，下面开始实现反向传播。通过反向传播来计算梯度——计算每层的w和b相当于成本函数的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_backward函数用于根据后一层的dZ来计算前面一层的dW，db和dA。也就是实现了下面3个公式\n",
    "$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dZ -- 后面一层的dZ\n",
    "    cache -- 前向传播时我们保存下来的关于本层的一些变量\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_backward用于根据本层的dA计算出本层的dZ。就是实现了下面的公式\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "上式的g'()表示求Z相当于本层的激活函数的偏导数。所以不同的激活函数也有不同的求导公式。\n",
    "我们为大家编写了两个求导函数sigmoid_backward和relu_backward。大家当前不需要关心这两个函数的内部实现，当然，如果你感兴趣可以到dnn_utils.py里面去看它们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dA -- 本层的dA \n",
    "    cache -- 前向传播时保存的本层的相关变量\n",
    "    activation -- 指示该层使用的是什么激活函数: \"sigmoid\" 或 \"relu\"\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    # 这里我们又顺带根据本层的dZ算出本层的dW和db以及前一层的dA\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面这个函数构建出整个反向传播。\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    AL -- 最后一层的A，也就是y'，预测出的标签\n",
    "    Y -- 真实标签\n",
    "    caches -- 前向传播时保存的每一层的相关变量，用于辅助计算反向传播\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # 获取神经网络层数。caches列表的长度就等于神经网络的层数\n",
    "    Y = Y.reshape(AL.shape) # 让真实标签的维度和预测标签的维度一致\n",
    "    \n",
    "    # 计算出最后一层的dA，前面文章我们以及解释过，最后一层的dA与前面各层的dA的计算公式不同，\n",
    "    # 因为最后一个A是直接作为参数传递到成本函数的，所以不需要链式法则而直接就可以求dA（A相当于成本函数的偏导数）\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # 计算最后一层的dW和db，因为最后一层使用的激活函数是sigmoid\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "                                                                                            dAL, \n",
    "                                                                                            current_cache,\n",
    "                                                                                            activation = \"sigmoid\")\n",
    "\n",
    "    # 计算前面L-1层到第一层的每层的梯度，这些层都使用relu激活函数\n",
    "    for c in reversed(range(1,L)): # reversed(range(1,L))的结果是L-1,L-2...1。是不包括L的。第0层是输入层，不必计算。\n",
    "        # 这里的c表示当前层\n",
    "        grads[\"dA\" + str(c-1)], grads[\"dW\" + str(c)], grads[\"db\" + str(c)] = linear_activation_backward(\n",
    "            grads[\"dA\" + str(c)], \n",
    "            caches[c-1],\n",
    "            # 这里我们也是需要当前层的caches，但是为什么是c-1呢？因为grads是字典，我们从1开始计数，而caches是列表，\n",
    "            # 是从0开始计数。所以c-1就代表了c层的caches。数组的索引很容易引起莫名其妙的问题，大家编程时一定要留意。\n",
    "            activation = \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的反向传播，我们得到了每一层的梯度（每一层w和b相当于成本函数的偏导数）。下面的update_parameters函数将利用这些梯度来更新/优化每一层的w和b，也就是进行梯度下降。\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- 每一层的参数w和b \n",
    "    grads -- 每一层的梯度\n",
    "    learning_rate -- 是学习率，学习步进\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # 获取层数。//除法可以得到整数\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print (\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print (\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print (\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，工具函数都编写好了，下面我们开始使用它们来构建一个深度神经网络以识别图片中是否有猫。\n",
    "首先，我们还是老规矩——第一步加载数据集。本次的数据集和《第一个人工智能程序》时用的是一样一样的。\n",
    "关于数据集的信息和加载方式可以看我的《第一个人工智能程序》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "m_train = train_x_orig.shape[0] # 训练样本的数量\n",
    "m_test = test_x_orig.shape[0] # 测试样本的数量\n",
    "num_px = test_x_orig.shape[1] # 每张图片的宽/高\n",
    "\n",
    "# 为了方便后面进行矩阵运算，我们需要将样本数据进行扁平化和转置\n",
    "# 处理后的数组各维度的含义是（图片数据，样本数）\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n",
    "\n",
    "# 下面我们对特征数据进行了简单的标准化处理（除以255，使所有值都在[0，1]范围内）\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用上面的工具函数构建一个深度神经网络训练模型\n",
    "def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    X -- 数据集\n",
    "    Y -- 数据集标签\n",
    "    layers_dims -- 指示该深度神经网络用多少层，每层有多少个神经元\n",
    "    learning_rate -- 学习率\n",
    "    num_iterations -- 指示需要训练多少次\n",
    "    print_cost -- 指示是否需要在将训练过程中的成本信息打印出来，好知道训练的进度好坏。\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 返回训练好的参数。以后就可以用这些参数来识别新的陌生的图片\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "\n",
    "    # 初始化每层的参数w和b\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # 按照指示的次数来训练深度神经网络\n",
    "    for i in range(0, num_iterations):\n",
    "        # 进行前向传播\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # 计算成本\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # 进行反向传播\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # 更新参数，好用这些参数进行下一轮的前向传播\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # 打印出成本\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print (\"训练%i次后成本是: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # 画出成本曲线图\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练100次后成本是: 0.672053\n",
      "训练200次后成本是: 0.648263\n",
      "训练300次后成本是: 0.611507\n",
      "训练400次后成本是: 0.567047\n",
      "训练500次后成本是: 0.540138\n",
      "训练600次后成本是: 0.527930\n",
      "训练700次后成本是: 0.465477\n",
      "训练800次后成本是: 0.369126\n",
      "训练900次后成本是: 0.391747\n",
      "训练1000次后成本是: 0.315187\n",
      "训练1100次后成本是: 0.272700\n",
      "训练1200次后成本是: 0.237419\n",
      "训练1300次后成本是: 0.199601\n",
      "训练1400次后成本是: 0.189263\n",
      "训练1500次后成本是: 0.161189\n",
      "训练1600次后成本是: 0.148214\n",
      "训练1700次后成本是: 0.137775\n",
      "训练1800次后成本是: 0.129740\n",
      "训练1900次后成本是: 0.121225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置好深度神经网络的层次信息——下面代表了一个4层的神经网络（12288是输入层），\n",
    "# 第一层有20个神经元，第二层有7个神经元。。。\n",
    "# 你也可以构建任意层任意神经元数量的神经网络，只需要更改下面这个数组就可以了\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# 根据上面的层次信息来构建一个深度神经网络，并且用之前加载的数据集来训练这个神经网络，得出训练后的参数\n",
    "parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们再编写一个预测函数，来用上面训练得到的参数来进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):   \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # 进行一次前向传播，得到预测结果\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    print(probas)\n",
    "    # 将预测结果转化成0和1的形式，即大于0.5的就是1，否则就是0\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11355187 0.11321288 0.99421715 0.11320249 0.11151952 0.11320249\n",
      "  0.11320249 0.99868565 0.11297217 0.11320249 0.11320249 0.99212492\n",
      "  0.11361743 0.99978505 0.99238936 0.11223783 0.11320249 0.11320249\n",
      "  0.11273694 0.99979337 0.11320249 0.11113007 0.11321129 0.11336202\n",
      "  0.99805632 0.99272884 0.11158027 0.99875585 0.11320249 0.98798578\n",
      "  0.11340662 0.11231844 0.11339752 0.11237601 0.11320249 0.11320488\n",
      "  0.11320249 0.11305355 0.95459455 0.11634576 0.11320249 0.97668361\n",
      "  0.99827849 0.11320249 0.11320249 0.11209139 0.11292575 0.98122606\n",
      "  0.11320249 0.11309981 0.96476361 0.11320249 0.11320249 0.11328443\n",
      "  0.99710096 0.11320249 0.99809563 0.98289964 0.11320249 0.99937053\n",
      "  0.99667978 0.98935349 0.11320249 0.11320249 0.11320249 0.11378944\n",
      "  0.11269344 0.11198864 0.11320249 0.11320249 0.11320249 0.99997463\n",
      "  0.11320249 0.11229775 0.11320249 0.15924221 0.11320249 0.11320249\n",
      "  0.11320249 0.11320249 0.11320249 0.11394959 0.11320249 0.99569335\n",
      "  0.98801701 0.11320249 0.11374373 0.11320249 0.97048389 0.11320249\n",
      "  0.11091985 0.1133149  0.11320249 0.99158351 0.99954149 0.11320249\n",
      "  0.11320249 0.98219517 0.11320249 0.11273604 0.11384284 0.11320249\n",
      "  0.99523984 0.11621143 0.99976858 0.11320249 0.99926581 0.95314957\n",
      "  0.96919655 0.99469097 0.99582645 0.99940756 0.11320249 0.11320249\n",
      "  0.11320249 0.11320249 0.11320249 0.99243892 0.11320249 0.11320249\n",
      "  0.11320249 0.95951483 0.11320249 0.11320249 0.27699502 0.11320249\n",
      "  0.99196707 0.11341575 0.99018699 0.99896568 0.12543501 0.11170539\n",
      "  0.11320249 0.99984857 0.98101634 0.9856538  0.99569001 0.97622991\n",
      "  0.11320249 0.11320249 0.11320249 0.11320249 0.99837671 0.11320249\n",
      "  0.98912318 0.99179754 0.99079338 0.11348789 0.98268212 0.99753093\n",
      "  0.11834557 0.11261488 0.11320249 0.99815318 0.11366585 0.11320249\n",
      "  0.99824494 0.11320249 0.11320249 0.11275918 0.11320249 0.11320249\n",
      "  0.99449718 0.11320249 0.99780506 0.11320249 0.99972335 0.11320249\n",
      "  0.11287302 0.99061965 0.98985401 0.99884966 0.11607315 0.11320249\n",
      "  0.99932897 0.99553378 0.11212132 0.99040699 0.11119629 0.9851028\n",
      "  0.11229247 0.11311966 0.11320249 0.11320249 0.11320249 0.98457229\n",
      "  0.11320249 0.11092261 0.9999191  0.11320249 0.11320249 0.11345908\n",
      "  0.99178288 0.11320249 0.11320249 0.11300832 0.12323144 0.99378729\n",
      "  0.11205717 0.11320249 0.37845867 0.11333768 0.11320249 0.11320249\n",
      "  0.11316266 0.11098908 0.11320249 0.11320249 0.11218756]]\n",
      "预测准确率是: 0.9808612440191385\n"
     ]
    }
   ],
   "source": [
    "# 对训练数据集进行预测\n",
    "pred_train = predict(train_x,parameters)\n",
    "print(\"预测准确率是: \" + str(np.sum((pred_train == train_y) / train_x.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99857185 0.9958758  0.97549937 0.81043887 0.97941449 0.54017501\n",
      "  0.78451331 0.99878998 0.98025811 0.98951161 0.9952642  0.4091547\n",
      "  0.98829682 0.99910113 0.11320249 0.99766381 0.26173268 0.99079338\n",
      "  0.80657707 0.13828897 0.9813595  0.12434335 0.11320249 0.74828188\n",
      "  0.95205147 0.98900063 0.94263045 0.11320249 0.21053047 0.99547312\n",
      "  0.95271361 0.99978218 0.99466482 0.97009512 0.83908259 0.11320249\n",
      "  0.12638741 0.6924672  0.91541111 0.11320249 0.93824396 0.96909619\n",
      "  0.82745341 0.11345511 0.95550664 0.99630299 0.55384169 0.99997405\n",
      "  0.60810654 0.19278617]]\n",
      "预测准确率是: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 对测试数据集进行预测\n",
    "pred_test = predict(test_x,parameters)\n",
    "print(\"预测准确率是: \"  + str(np.sum((pred_test == test_y) / test_x.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，比我们之前的的单神经元网络的0.7提升了0.1。不要小看这0.1，这是很难提升的。别说是0.1，如果全世界都只能做到0.9，如果你能提升0.05，那么你就享誉全球啦！\n",
    "\n",
    "当然，0.8还不是我们的极限，后面的文章我会带领大家继续提升它。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"convolutional-neural-networks","graded_item_id":"qO8ng","launcher_item_id":"7XDi8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"14_01_CNN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-guPNRdzLj9","executionInfo":{"status":"ok","timestamp":1651995785282,"user_tz":-480,"elapsed":52092,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"be467c76-cc5f-4442-e38c-7c09a0c1c02c"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/explore/ai/14_16_cnn\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"8lg7iaCkbqxY"},"source":["# 手把手教你构建CNN（一）\n","\n","欢迎来到智能视觉班的第一个实战编程！本次我们将用numpy库来实现卷积层和池化层，以及前向传播和反向传播。\n","\n","**提示**:\n","- 上标 $[l]$ 表示属于第几层。 \n","    - 例如: $a^{[4]}$ 表示第4层的激活值。 $W^{[5]}$ 和 $b^{[5]}$ 表示第5层的参数。\n","\n","- 上标 $(i)$ 表示第i个样本。 \n","    - 例如: $x^{(i)}$表示第i个样本的输入特征。\n","    \n","    \n","- 下标 $i$ 表示向量的第i个元素。\n","    - 例如: $a^{[l]}_i$ 表示第l层的第i个激活值。\n","    \n","    \n","- $n_H$, $n_W$ 和 $n_C$ 表示某一层相关矩阵的高，宽以及深度。如果你想指明第l层相关矩阵的高，宽和深度，那么你可以写成如下形式 $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n","- $n_{H_{prev}}$, $n_{W_{prev}}$ 和 $n_{C_{prev}}$ 表示前一层相关矩阵的高宽和深度。当然，如果你想表示l层前面一层的高宽深度，也可以写成如下形式 $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$。"]},{"cell_type":"markdown","metadata":{"id":"Hrf3GBUkbqxb"},"source":["## 1 - 导入工具库"]},{"cell_type":"code","metadata":{"id":"SpuVsTEobqxc","executionInfo":{"status":"ok","timestamp":1651995786747,"user_tz":-480,"elapsed":1471,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}}},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) \n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nErtm35jbqxd"},"source":["## 2 - 概述\n","\n","在本次实战编程中我们将学会如何实现CNN相关的一些基本函数:\n","\n","- 卷积相关的函数:\n","    - 零填补（Zero Padding）\n","    - 卷积窗口(Convolve window )\n","    - 前向卷积（Convolution forward）\n","    - 反向卷积（Convolution backward）\n","- 池化相关的函数:\n","    - 前向池化（Pooling forward）\n","    - 创建掩码（Create mask ）\n","    - Distribute value\n","    - 反向池化（Pooling backward）\n","    \n","本次实战编程我们会使用numpy库来手工实现这些函数。下一个实战编程我们会用TensorFlow，当然，TensorFlow中已经帮我们实现了这些函数，所以，下一个实战编程的重点在于调用这些函数来构建出下面的CNN。\n","\n","<img width=700 src=\"https://z3.ax1x.com/2021/09/10/hjitOK.png\">"]},{"cell_type":"markdown","metadata":{"id":"Z1oOSTflbqxe"},"source":["## 3 - 卷积层\n","\n","卷积层会把一个矩阵转化成另一个不同尺寸的矩阵，如下图所示 \n","\n","<img width=350 src=\"https://z3.ax1x.com/2021/09/10/hjlVqH.png\">\n","\n","下面我们就来一步一步地实现卷积层。"]},{"cell_type":"markdown","metadata":{"id":"3WT7RPT-bqxf"},"source":["### 3.1 - 零填补（Zero-Padding）\n","\n","零填补就是往输入矩阵四周添加0,如下图所示:\n","<center><img width=600 height=400 src=\"https://z3.ax1x.com/2021/09/10/hjl4SK.png\"><caption><br><b><font color='#00c752'><u> 图 1</u> : Zero-Padding</b><br>上图的padding数量为2</caption></center>\n","\n","padding的作用如下:\n","\n","- 它可以让我们既使用卷积层又可以避免矩阵的尺寸变得越来越小。对于很深的神经网络来说意义很大，因为如果不使用padding，那么每卷积一次矩阵尺寸就会变小，这样一来等到很深的网络层时，矩阵就没啦~ \n","\n","- 它有助于保留图像四周的信息。因为如果没有padding，那么图像四周的元素被卷积的次数就很少。"]},{"cell_type":"code","metadata":{"id":"nWmf_sRubqxf","executionInfo":{"status":"ok","timestamp":1651995786748,"user_tz":-480,"elapsed":17,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}}},"source":["def zero_pad(X, pad):\n","    \"\"\"\n","    给样本集X的所有样本进行零填补。\n","    \n","    参数:\n","    X -- 样本集，维度是(m, n_H, n_W, n_C) ,\n","         m表示样本的数量，这里的样本是图片数据，n_H, n_W, n_C表示图片的高，宽，深度。\n","    pad -- 表示padding的个数，就是我们教程里说的p的数量。\n","    \n","    返回值:\n","    X_pad -- 返回填补后的样本集。维度是(m, n_H + 2*pad, n_W + 2*pad, n_C),每张图片的四周都填补了pad个0\n","    \"\"\"\n","    \n","    # np.pad是numpy提供的一个零填补函数，下面代码给X的n_H和n_W这两个维度填补pad个零。对m和n_C的维度不进行填补\n","    # 例如第一组(pad, pad)表示给图像的上面和下面都填补pad个零。当然，上面和下面也可以填充不同数量的零。\n","    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n","    \n","    return X_pad"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ONubYC4bqxg","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"ok","timestamp":1651995786748,"user_tz":-480,"elapsed":15,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"0191760b-1ec9-4c40-950b-559e98f41988"},"source":["# 单元测试\n","np.random.seed(1)\n","x = np.random.randn(4, 3, 3, 2)\n","x_pad = zero_pad(x, 2)\n","print (\"x.shape =\", x.shape)\n","print (\"x_pad.shape =\", x_pad.shape)\n","print (\"x[1, 1] =\", x[1, 1])\n","# print (\"x_pad[1, 1] =\", x_pad[1, 1])\n","print (\"x_pad[1, 3] =\", x_pad[1, 3])\n","\n","\n","fig, axarr = plt.subplots(1, 3)\n","axarr[0].set_title('x')\n","axarr[0].imshow(x[0,:,:,0])\n","axarr[1].set_title('x_pad')\n","axarr[1].imshow(x_pad[0,:,:,0])\n","axarr[2].set_title('x_pad2')\n","axarr[2].imshow(x_pad[2,:,:,0])"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape = (4, 3, 3, 2)\n","x_pad.shape = (4, 7, 7, 2)\n","x[1, 1] = [[ 0.90085595 -0.68372786]\n"," [-0.12289023 -0.93576943]\n"," [-0.26788808  0.53035547]]\n","x_pad[1, 3] = [[ 0.          0.        ]\n"," [ 0.          0.        ]\n"," [ 0.90085595 -0.68372786]\n"," [-0.12289023 -0.93576943]\n"," [-0.26788808  0.53035547]\n"," [ 0.          0.        ]\n"," [ 0.          0.        ]]\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f88b9dfbf10>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x288 with 3 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"-Chr20Psbqxi"},"source":["### 3.2 - 单步卷积\n","\n","下面我们实现单步卷积函数。这个函数只卷积一步。后面我们将调用这个函数来完成对整个输入矩阵的卷积操作。如下图所示，卷积层会对输入矩阵进行很多步卷积。每卷积一步就会得出一个输出矩阵的元素。\n","\n","<center><img width=500 src=\"https://z3.ax1x.com/2021/09/10/hjlXfP.gif\">\n","<caption><br><u><font color='#00c752'><b>图 2</u>: 卷积</b><br> 上面是一个3x3的过滤器，卷积步长是1 </caption></center>"]},{"cell_type":"code","metadata":{"id":"34tVnM0wbqxi","executionInfo":{"status":"ok","timestamp":1651995786749,"user_tz":-480,"elapsed":12,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}}},"source":["def conv_single_step(a_slice_prev, W, b):\n","    \"\"\"\n","    这个函数只执行一步卷积\n","    \n","    参数:\n","    a_slice_prev -- 输入矩阵中的一小块数据，如上面的动图所示，过滤器每次只与矩阵中的一小块数据进行卷积。\n","                 -- 这里的输入矩阵也就是上一层的输出矩阵。维度是(f, f, n_C_prev)\n","    W -- 权重参数w。其实这里就是指过滤器。过滤器就是权重参数w。\n","      -- 维度是(f, f, n_C_prev)，与a_slice_prev是一样的。因为是它俩进行卷积，所以维度肯定是一样的。\n","    b -- 阈值b，教程中我们说过每一个过滤器会有一个对应的阈值。 维度是(1, 1, 1)\n","    \n","    返回值:\n","    Z -- 卷积一步后得到的一个数值。这个数值将是输出矩阵中的一个元素。\n","    \"\"\"\n","\n","    # 将a_slice_prev与W的每一个元素进行相乘\n","    s = np.multiply(a_slice_prev, W) + b\n","    # 将上面相乘的结果累加起来\n","    Z = np.sum(s)\n","\n","    return Z"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"54i7-cO3bqxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651995786749,"user_tz":-480,"elapsed":11,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"46d4a2ac-2f1e-461f-8218-4ed3a86ec264"},"source":["np.random.seed(1)\n","a_slice_prev = np.random.randn(4, 4, 3)\n","W = np.random.randn(4, 4, 3)\n","b = np.random.randn(1, 1, 1)\n","\n","Z = conv_single_step(a_slice_prev, W, b)\n","print(\"Z =\", Z)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = -23.16021220252078\n"]}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"SAyTHItibqxj"},"source":["### 3.3 - 前向传播\n","\n","我们可以使用多个过滤器。每个过滤器卷积之后都会得到一个二维的矩阵。然后我们会将这些二维的矩阵叠加起来成为多维的矩阵。如下面的短视频所示。视频中有2个过滤器。\n","\n","<center><video width=620 height=440 src=\"https://user-images.githubusercontent.com/50534516/132989026-e0da4807-bffe-454a-9a0a-2306f59be03d.mp4\" type=\"video/mp4\"/></center>\n","\n","\n","**提示**: \n","1. 在python中，通过开始索引和结束索引，可以获取数组中某一区域的元素。例如，如果你想获取一个(5,5,3)矩阵中的左上角的一个2x2的子矩阵，那么你可以使用下面的代码:\n","```python\n","a_slice_prev = a_prev[0:2,0:2,:]\n","```\n","在下面的函数中，我们会用这一语法来从输入矩阵中获取一小块数据来与过滤器进行卷积。\n","2. 为了从矩阵中获取一个子矩阵（一小块数据），我们首先需要计算出子矩阵在母矩阵中的坐标，纵向开始索引`vert_start`, 纵向结束索引`vert_end`, 横向开始索引`horiz_start` 和横向结束索引 `horiz_end`。然后就可以用这些索引来定位一个子矩阵，如下图所示。\n","\n","<center><img width=400 src=\"https://user-images.githubusercontent.com/50534516/132990374-55befbc1-ba6f-4fe1-b1a0-dfda5aeecf23.png\">\n","<caption><br><u><font color='#00c752'> 图 3 </u>: 定位子矩阵 <br> </center></caption>\n","\n","\n","$$函数中还会用到如下公式来计算输出矩阵的维度:$$\n","$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n","$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n","$$ n_C = \\text{这个是过滤器的个数}$$"]},{"cell_type":"code","metadata":{"id":"MI6JOBd3bqxk","executionInfo":{"status":"ok","timestamp":1651995786750,"user_tz":-480,"elapsed":10,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}}},"source":["def conv_forward(A_prev, W, b, hparameters):\n","    \"\"\"\n","    实现卷积网络的前向传播\n","    \n","    参数:\n","    A_prev -- 本层的输入矩阵，也就是上一层的输出矩阵。维度是(m, n_H_prev, n_W_prev, n_C_prev)\n","    W -- 权重，也就是过滤器。维度是 (f, f, n_C_prev, n_C)。后面的n_C表示过滤器的个数\n","    b -- 阈值。维度是 (1, 1, 1, n_C)。一个过滤器配一个阈值。所以最后一维也是n_C\n","    hparameters -- 超参数步长s和padding数p\n","        \n","    返回值:\n","    Z -- 输出矩阵，也就是卷积结果。维度是(m, n_H, n_W, n_C)\n","    cache -- 缓存一些数值，以供反向传播时用。\n","    \"\"\"\n","    \n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    \n","    (f, f, n_C_prev, n_C) = W.shape\n","\n","    stride = hparameters['stride'] # 步长s\n","    pad = hparameters['pad'] # 填补数量p\n","    \n","    # 计算输出矩阵的维度。参考上面提供的公式    \n","    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1 # 使用int()来实现向下取整\n","    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n","    \n","    # 初始化输出矩阵\n","    Z = np.zeros((m, n_H, n_W, n_C))\n","    \n","    # 给输入矩阵进行padding填补0\n","    A_prev_pad = zero_pad(A_prev, pad)\n","    \n","    for i in range(m):                                 # 遍历每一个样本\n","        a_prev_pad = A_prev_pad[i]                     # 取出一个样本对应的输入矩阵\n","        for h in range(n_H):                           # 遍历输出矩阵的高\n","            for w in range(n_W):                       # 遍历输出矩阵的宽\n","                for c in range(n_C):                   # 遍历每一个过滤器\n","                    # 计算出输入矩阵中本次应该卷积的区域的索引，然后通过这些索引取出将被卷积的小块数据。\n","                    vert_start = h * stride\n","                    vert_end = vert_start + f\n","                    horiz_start = w * stride\n","                    horiz_end = horiz_start + f\n","                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n","                    # 利用之前我们实现的conv_single_step函数来对这块数据进行卷积。\n","                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n","                                        \n","    assert(Z.shape == (m, n_H, n_W, n_C))\n"," \n","    cache = (A_prev, W, b, hparameters)\n","    \n","    return Z, cache"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykUpbuabbqxk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651995786750,"user_tz":-480,"elapsed":10,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"48e0e8ee-b134-483d-ce96-a4fef48eea99"},"source":["np.random.seed(1)\n","A_prev = np.random.randn(10, 4, 4, 3)\n","W = np.random.randn(2, 2, 3, 8)\n","b = np.random.randn(1, 1, 1, 8)\n","hparameters = {\"pad\" : 2,\n","               \"stride\": 1}\n","\n","Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n","print(\"Z's mean =\", np.mean(Z))\n","print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Z's mean = 0.15585932488906465\n","cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"]}]},{"cell_type":"markdown","metadata":{"id":"hJvFKeuTbqxl"},"source":["下一篇文档我们继续学习如何实现池化层和反向传播！"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"convolutional-neural-networks","graded_item_id":"qO8ng","launcher_item_id":"7XDi8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"14_02_CNN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"TNeGaWKL7R6R","executionInfo":{"status":"ok","timestamp":1651999316591,"user_tz":-480,"elapsed":809,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/explore/ai/14_16_cnn\")"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJD_4hjZ60Le"},"source":["我们先把上一篇文档的代码贴出来"]},{"cell_type":"code","metadata":{"id":"gyyREXgK60Lj"},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0)\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQCKcNmQ60Ll"},"source":["def zero_pad(X, pad):\n","    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n","    return X_pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"waVk-xNZ60Lm","executionInfo":{"status":"ok","timestamp":1651995860005,"user_tz":-480,"elapsed":865,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"26a0b12a-9149-45d6-f258-8599356ac30c"},"source":["np.random.seed(1)\n","x = np.random.randn(4, 3, 3, 2)\n","x_pad = zero_pad(x, 2)\n","print (\"x.shape =\", x.shape)\n","print (\"x_pad.shape =\", x_pad.shape)\n","print (\"x[1, 1] =\", x[1, 1])\n","print (\"x_pad[1, 1] =\", x_pad[1, 1])\n","\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x')\n","axarr[0].imshow(x[0,:,:,0])\n","axarr[1].set_title('x_pad')\n","axarr[1].imshow(x_pad[0,:,:,0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape = (4, 3, 3, 2)\n","x_pad.shape = (4, 7, 7, 2)\n","x[1, 1] = [[ 0.90085595 -0.68372786]\n"," [-0.12289023 -0.93576943]\n"," [-0.26788808  0.53035547]]\n","x_pad[1, 1] = [[0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]]\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fd8046a5f50>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"dtV66gYd60Lm"},"source":["def conv_single_step(a_slice_prev, W, b):\n","    s = np.multiply(a_slice_prev, W) + b\n","\n","    Z = np.sum(s)\n","\n","    return Z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABE3kevb60Ln","executionInfo":{"status":"ok","timestamp":1651995860006,"user_tz":-480,"elapsed":18,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"62e2c028-c836-49d8-e1ab-114931d32cc5"},"source":["np.random.seed(1)\n","a_slice_prev = np.random.randn(4, 4, 3)\n","W = np.random.randn(4, 4, 3)\n","b = np.random.randn(1, 1, 1)\n","\n","Z = conv_single_step(a_slice_prev, W, b)\n","print(\"Z =\", Z)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = -23.16021220252078\n"]}]},{"cell_type":"code","metadata":{"id":"5aMP1IZq60Lo"},"source":["def conv_forward(A_prev, W, b, hparameters):\n","\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    \n","    (f, f, n_C_prev, n_C) = W.shape\n","\n","    stride = hparameters['stride']\n","    pad = hparameters['pad']\n","    \n","    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n","    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n","\n","    Z = np.zeros((m, n_H, n_W, n_C))\n"," \n","    A_prev_pad = zero_pad(A_prev, pad)\n","    \n","    for i in range(m):                                 \n","        a_prev_pad = A_prev_pad[i]                   \n","        for h in range(n_H):                         \n","            for w in range(n_W):                    \n","                for c in range(n_C): \n","                    vert_start = h * stride\n","                    vert_end = vert_start + f\n","                    horiz_start = w * stride\n","                    horiz_end = horiz_start + f\n","                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n","                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n"," \n","    assert(Z.shape == (m, n_H, n_W, n_C))\n","    cache = (A_prev, W, b, hparameters)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCDII56k60Lo","executionInfo":{"status":"ok","timestamp":1651995860007,"user_tz":-480,"elapsed":15,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"9fd25f86-5f45-44a5-8248-9762ac1196b9"},"source":["np.random.seed(1)\n","A_prev = np.random.randn(10, 4, 4, 3)\n","W = np.random.randn(2, 2, 3, 8)\n","b = np.random.randn(1, 1, 1, 8)\n","hparameters = {\"pad\" : 2,\n","               \"stride\": 1}\n","\n","Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n","print(\"Z's mean =\", np.mean(Z))\n","print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Z's mean = 0.15585932488906465\n","cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"]}]},{"cell_type":"markdown","metadata":{"id":"E4S8Pm2T60Lp"},"source":["## 4 - 池化层\n","\n","池化层可以将矩阵的尺寸变小。池化层不仅仅可以降低神经网络的计算量，还可以让网络的预测鲁棒性更好。\n","\n","之前我给大家介绍了两种池化层: \n","\n","- 最大池化层（Max-pooling layer）: 取输入矩阵的子矩阵中的最大值元素作为输出矩阵的一个元素。\n","\n","- 平均池化层（Average-pooling layer）: 取输入矩阵的子矩阵中的元素的平均值作为输出矩阵的一个元素。“子矩阵”在这里也被叫做“窗口”。\n","\n","<table>\n","<td>\n","<img width=500 src=\"https://user-images.githubusercontent.com/50534516/132990764-58614d23-fbc8-40c3-8a65-6bfec5f30d86.png\">\n","<td>\n","<td>\n","<img width=500 src=\"https://user-images.githubusercontent.com/50534516/132990867-808024b2-e8da-48cf-aa06-3ca2f69dbd56.png\">\n","<td>\n","</table>\n","\n","池化层是没有参数的，因为它的过滤器是虚拟的是不存在的。但是它有超参数，例如窗口大小f，也就是指定池化的子矩阵的高宽。 \n","\n","### 4.1 - 池化的前向传播\n","下面我们实现了一个池化的前向传播函数，它可以指定用最大池化还是平均池化。\n","\n","**提示**:\n","因为池化没有padding，所以计算输出矩阵的维度公式会与卷积层的有所不同:\n","$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n","$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n","$$ n_C = n_{C_{prev}}$$"]},{"cell_type":"code","metadata":{"id":"XfMVQaZo60Lr"},"source":["def pool_forward(A_prev, hparameters, mode = \"max\"):\n","    \"\"\"\n","    参数:\n","    A_prev -- 输入矩阵，也就是上一层的输出矩阵。维度是(m, n_H_prev, n_W_prev, n_C_prev)\n","    hparameters -- 超参数窗口大小f和步长s\n","    mode -- 池化模式,最大池化就写max，如果想用平均池化，就写average。\n","    \n","    返回值:\n","    A -- 池化层的输出矩阵，维度是(m, n_H, n_W, n_C)\n","    cache -- 缓存一些数据\n","    \"\"\"\n","\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    \n","    f = hparameters[\"f\"] # 窗口大小\n","    stride = hparameters[\"stride\"] # 步长\n","    \n","    # 计算输出矩阵的尺寸大小\n","    n_H = int(1 + (n_H_prev - f) / stride)\n","    n_W = int(1 + (n_W_prev - f) / stride)\n","    n_C = n_C_prev\n","    \n","    # 初始化输出矩阵\n","    A = np.zeros((m, n_H, n_W, n_C))    \n","    \n","    for i in range(m):                           # 遍历所有样本\n","        for h in range(n_H):                     # 纵向遍历输出矩阵\n","            for w in range(n_W):                 # 横向遍历输出矩阵\n","                for c in range (n_C):            # 遍历输出矩阵的深度\n","                    \n","                    # 计算出输入矩阵中本次应该本池化的区域的索引，也就是本次的池化窗口的索引\n","                    vert_start = h * stride\n","                    vert_end = vert_start + f\n","                    horiz_start = w * stride\n","                    horiz_end = horiz_start + f\n","                    \n","                    # 通过上面的索引取出将被池化的子矩阵窗口\n","                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n","                    \n","                    # 执行池化\n","                    if mode == \"max\":\n","                        A[i, h, w, c] = np.max(a_prev_slice) # np.max就是numpy库中求最大值的函数\n","                    elif mode == \"average\":\n","                        A[i, h, w, c] = np.mean(a_prev_slice)# 求平均值\n","\n","    cache = (A_prev, hparameters)\n","\n","    assert(A.shape == (m, n_H, n_W, n_C))\n","    \n","    return A, cache\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZLgaeN960Lu","executionInfo":{"status":"ok","timestamp":1651995860008,"user_tz":-480,"elapsed":13,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"e3d70321-42b8-4072-a43c-73e0af1cd1dc"},"source":["# 单元测试\n","np.random.seed(1)\n","A_prev = np.random.randn(2, 4, 4, 3)\n","hparameters = {\"stride\" : 1, \"f\": 4}\n","\n","A, cache = pool_forward(A_prev, hparameters)\n","print(\"mode = max\")\n","print(\"A =\", A)\n","print()\n","A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n","print(\"mode = average\")\n","print(\"A =\", A)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mode = max\n","A = [[[[1.74481176 1.6924546  2.10025514]]]\n","\n","\n"," [[[1.19891788 1.51981682 2.18557541]]]]\n","\n","mode = average\n","A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n","\n","\n"," [[[-0.09525108  0.28325018  0.33035185]]]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"vk6X6QVc60Lu"},"source":["至此，我们已经实现了CNN的所有前向传播操作。下面我们再来实现它的反向传播。"]},{"cell_type":"markdown","metadata":{"id":"avFLc3Li60Lv"},"source":["## 5 - CNN的反向传播\n","\n","如果我们使用深度学习框架，例如Tensorflow，keras等等，我们只需要实现前向传播就可以了，因为框架会自动帮我们实现反向传播。因此，很多人工智能工程师都不需要关心反向传播的细节。\n","\n","CNN的反向传播是比较复杂的。下面我也只是大概的介绍一下它。大家也就大致了解一下就可以了，看不懂也没有关系。\n","\n","在我们之前学过的神经网络反向传播中，我们会通过计算成本函数相关的偏导数来不断更新参数w和b的值。其实在CNN的反向传播也是这个流程。在教程中我们没有列出CNN反向传播的相关公式，下面我会简单地给大家介绍一下它们。\n","\n","### 5.1 - 卷积层的反向传播\n","\n","首先我们来学习一下如何实现卷积层的反向传播。\n","\n","#### 5.1.1 - 计算dA:\n","下面的公式被用来计算某个样本的某个过滤器的dA:\n","\n","$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n","\n","$W_c$是表示这个过滤器。公式看不懂没关系！\n","\n","这个公式对应的python代码如下：\n","```python\n","da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n","```\n","\n","#### 5.1.2 - 计算dW:\n","下面的公式被用来计算某个过滤器的dW:\n","\n","$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n","\n","$a_{slice}$表示输入矩阵中的被卷积的子矩阵。\n","\n","上面的公式对应于下面的python代码:\n","```python\n","dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n","```\n","\n","#### 5.1.3 - 计算db:\n","\n","这个公式用来计算某个过滤器的db:\n","\n","$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n","\n","上面的公式对应于下面的python代码:\n","```python\n","db[:,:,:,c] += dZ[i, h, w, c]\n","```"]},{"cell_type":"code","metadata":{"id":"cmVZN1hf60Lv"},"source":["def conv_backward(dZ, cache):\n","    \"\"\"    \n","    参数:\n","    dZ -- 后一层相关的dZ，维度是(m, n_H, n_W, n_C)\n","    cache -- 前面的conv_forward()函数保存下来的缓存数据\n","    \n","    Returns:\n","    dA_prev -- 本卷积层输入矩阵的dA，维度是(m, n_H_prev, n_W_prev, n_C_prev)\n","    dW -- 本卷积层相关的dW,维度是(f, f, n_C_prev, n_C)\n","    db -- 本卷积层相关的db,维度是(1, 1, 1, n_C)\n","    \"\"\"\n","\n","    (A_prev, W, b, hparameters) = cache\n","    \n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","    \n","    (f, f, n_C_prev, n_C) = W.shape\n","    \n","    stride = hparameters[\"stride\"] # 步长\n","    pad = hparameters[\"pad\"] # padding数量\n","    \n","    (m, n_H, n_W, n_C) = dZ.shape\n","    \n","    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n","    dW = np.zeros((f, f, n_C_prev, n_C))\n","    db = np.zeros((1, 1, 1, n_C))\n","\n","    A_prev_pad = zero_pad(A_prev, pad)\n","    dA_prev_pad = zero_pad(dA_prev, pad)\n","    \n","    for i in range(m):                       # 遍历每一个样本\n","        \n","        a_prev_pad = A_prev_pad[i]\n","        da_prev_pad = dA_prev_pad[i]\n","        \n","        for h in range(n_H):                   # 遍历输出矩阵的高\n","            for w in range(n_W):               # 遍历输出矩阵的宽\n","                for c in range(n_C):           # 遍历输出矩阵的深度\n","                    \n","                    # 计算输入矩阵中的子矩阵的索引\n","                    vert_start = h\n","                    vert_end = vert_start + f\n","                    horiz_start = w\n","                    horiz_end = horiz_start + f\n","                    \n","                    # 取出当前进行卷积的子矩阵\n","                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n","\n","                    # 用上面的公式来计算偏导数\n","                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n","                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n","                    db[:,:,:,c] += dZ[i, h, w, c]\n","                    \n","        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n","\n","    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Muu3VAt760Lw","executionInfo":{"status":"ok","timestamp":1651995860011,"user_tz":-480,"elapsed":14,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"8960c290-953e-41ba-9c47-486e488da8a9"},"source":["np.random.seed(1)\n","dA, dW, db = conv_backward(Z, cache_conv)\n","print(\"dA_mean =\", np.mean(dA))\n","print(\"dW_mean =\", np.mean(dW))\n","print(\"db_mean =\", np.mean(db))\n","# print(dA.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dA_mean = 9.608990675868995\n","dW_mean = 10.581741275547563\n","db_mean = 76.37106919563735\n"]}]},{"cell_type":"markdown","metadata":{"id":"Nqnm-ZNN60Lw"},"source":["## 5.2 池化层的反向传播\n","\n","接下来我们实现一下池化层的反向传播。虽然池化层没有参数需要被更新，但是在编程时我们依然需要实现它的反向传播，这样才能继续计算在这个池化层前面的那些层的反向传播。\n","\n","### 5.2.1 最大池化的反向传播\n","\n","在实现池化层之前，我们需要先实现两个工具函数，第一个是 `create_mask_from_window()`，它可以根据输入矩阵得到一个特殊的输出矩阵，这个输出矩阵中只有最大值处是1，其余都是零。如下所示，X是输入矩阵，M是函数的输出矩阵: \n","\n","$$ X = \\begin{bmatrix}\n","1 && 3 \\\\\n","4 && 2\n","\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n","0 && 0 \\\\\n","1 && 0\n","\\end{bmatrix}\\tag{4}$$\n","\n","提示:\n","- [np.max()]()会返回矩阵中的最大元素。\n","- python语法`A = (X == x)`会生成一个矩阵A，这个A与X的维度是一样的，A里面其它元素都为0，只有与小x的值相同的位置处为1，也就是为True。python中0等于False，1等于True:\n","```\n","A[i,j] = True if X[i,j] = x\n","A[i,j] = False if X[i,j] != x\n","```"]},{"cell_type":"code","metadata":{"id":"xDh0Rmk_60Lx"},"source":["def create_mask_from_window(x):\n","\n","    # x是一个矩阵。np.max(x)会得到最大元素。\n","    # mask是一个与x维度相同的矩阵，里面其余元素都为0，只有x最大值元素的位置处为1\n","    mask = x == np.max(x)\n","    \n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Hh1NGj1i60Ly","executionInfo":{"status":"ok","timestamp":1651995860797,"user_tz":-480,"elapsed":12,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"1ff2c178-6894-4353-de6d-aad72d533810"},"source":["np.random.seed(1)\n","x = np.random.randn(2,3)\n","mask = create_mask_from_window(x)\n","print('x = ', x)\n","print(\"mask = \", mask)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  [[ 1.62434536 -0.61175641 -0.52817175]\n"," [-1.07296862  0.86540763 -2.3015387 ]]\n","mask =  [[ True False False]\n"," [False False False]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"al587lVf60Ly"},"source":["### 5.2.2 - 平均池化的反向传播\n","\n","为了实现最大池化的反向传播，我们需要实现如下的工具函数`distribute_value`。就是把一个数值平分成一个矩阵，例如把1平分成四分之一到一个矩阵中: \n","$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n","1/4 && 1/4 \\\\\n","1/4 && 1/4\n","\\end{bmatrix}\\tag{5}$$"]},{"cell_type":"code","metadata":{"id":"sUUxk2Nk60Lz"},"source":["def distribute_value(dz, shape):\n","    \"\"\"    \n","    参数:\n","    dz -- 一个数值\n","    shape -- 输出矩阵的维度\n","    \n","    返回值:\n","    a -- a的维度就是shape，里面的值是又dz平分而来的\n","    \"\"\"\n","    (n_H, n_W) = shape\n","    \n","    # 计算平均值\n","    average = dz / (n_H * n_W)\n","    \n","    # 构建输出矩阵\n","    a = np.ones(shape) * average\n","    \n","    return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrxYVNXA60Lz","executionInfo":{"status":"ok","timestamp":1651995860798,"user_tz":-480,"elapsed":10,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"2b4bf6eb-5840-4daa-f110-8b96c2300810"},"source":["a = distribute_value(2, (2,2))\n","print('distributed value =', a)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["distributed value = [[0.5 0.5]\n"," [0.5 0.5]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"rgthy3lA60Lz"},"source":["### 5.2.3 池化层的反向传播\n","\n","接下来我们将上面的工具函数组合起来实现池化层的反向传播。"]},{"cell_type":"code","metadata":{"id":"NH946i0560L0"},"source":["\n","def pool_backward(dA, cache, mode = \"max\"):\n","    \"\"\"\n","    参数:\n","    dA -- 本池化层的输出矩阵对应的偏导数\n","    cache -- 前向传播时缓存起来的数值\n","    mode -- 是最大池化还是平均池化，(\"max\" 或 \"average\")\n","    \n","    Returns:\n","    dA_prev -- 本池化层的输入矩阵对应的偏导数\n","    \"\"\"\n","\n","    # A_prev是本池化层的输入矩阵\n","    (A_prev, hparameters) = cache\n","    \n","    stride = hparameters[\"stride\"]\n","    f = hparameters[\"f\"]\n","    \n","    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n","    m, n_H, n_W, n_C = dA.shape\n","    \n","    dA_prev = np.zeros(A_prev.shape)\n","    \n","    for i in range(m):                     \n","        a_prev = A_prev[i]\n","        for h in range(n_H):                  \n","            for w in range(n_W):             \n","                for c in range(n_C):          \n","                    vert_start = h\n","                    vert_end = vert_start + f\n","                    horiz_start = w\n","                    horiz_end = horiz_start + f\n","                    \n","                    if mode == \"max\":\n","                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n","                        mask = create_mask_from_window(a_prev_slice)\n","                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n","                        \n","                    elif mode == \"average\":\n","                        da = dA[i, h, w, c]\n","                        shape = (f, f)\n","                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n","                        \n","    assert(dA_prev.shape == A_prev.shape)\n","    \n","    return dA_prev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSSbU6YE60L0","executionInfo":{"status":"ok","timestamp":1651995860799,"user_tz":-480,"elapsed":8,"user":{"displayName":"yuxiang wang","userId":"09333219890846106757"}},"outputId":"e29c88d9-6e51-4299-8dc0-c2c8617e8918"},"source":["np.random.seed(1)\n","A_prev = np.random.randn(5, 5, 3, 2)\n","hparameters = {\"stride\" : 1, \"f\": 2}\n","A, cache = pool_forward(A_prev, hparameters)\n","dA = np.random.randn(5, 4, 2, 2)\n","\n","dA_prev = pool_backward(dA, cache, mode = \"max\")\n","print(\"mode = max\")\n","print('mean of dA = ', np.mean(dA))\n","print('dA_prev[1,1] = ', dA_prev[1,1])  \n","print()\n","dA_prev = pool_backward(dA, cache, mode = \"average\")\n","print(\"mode = average\")\n","print('mean of dA = ', np.mean(dA))\n","print('dA_prev[1,1] = ', dA_prev[1,1]) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mode = max\n","mean of dA =  0.14571390272918056\n","dA_prev[1,1] =  [[ 0.          0.        ]\n"," [ 5.05844394 -1.68282702]\n"," [ 0.          0.        ]]\n","\n","mode = average\n","mean of dA =  0.14571390272918056\n","dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n"," [ 1.26461098 -0.25749373]\n"," [ 1.17975636 -0.53624893]]\n"]}]}]}